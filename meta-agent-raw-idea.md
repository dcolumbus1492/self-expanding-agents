about a year ago i've posted a blog post on LinkedIn about dynamic agents: 
---
Tl;dr: next generation of AI agents will have dynamic tool generation capabilities. 

Current agentic AI systems (e.g. ChatGPT, Claude) are awesome, and we've just started to scratch the surface of their real-world usefulness. 
However, they are still inherently constrained by their pre-defined set of "tools", i.e., the functions that the agent can execute.
In other words, the ceiling of an agentic system is always determined by the existence and quality of the tools it has access to.
In this sense, an agent is an entity that is designed to operate within a specific context/space which is narrowed down by its toolset.

This leads me to think that the next generation of AI agents will likely be able to generate their own tools on-demand, per situation.
Such a system would unlock endless possibilities by increasing its autonomy dramatically and eliminating the need for defining tools in advance.

Now, from an engineering perspective, how can we build such systems? 
To answer this, we need to figure out two things: 
a. what are the components of a tool? 
b. can it be automatically generated by AI?

Generally speaking, a tool is enabled by the following:
1. Code: Representing the tool's logic and functionality, wrapped as a function.
2. Schema: Defining the function's parameters.
3. Function/Tool Calling mechanism: Enabling tool execution by extracting the function's arguments from the agent's context.

Can we automatically generate these?

The answer is simple, yes:
- LLMs are not only capable of but excel at generating code from natural language descriptions.
- The schemas can be derived directly from the function's code (or be generated using LLMs in trickier cases).
- Function calling mechanisms are now supported in most LLMs.

In fact, there is nothing essential that prevents us from building such systems today (which is a bit scary).
The technology is already here, waiting to be unleashed. 

Interesting times ahead...
---

now, inpired by my ideas, and given how the tech is now cooked enough to support this, i'm thinking about how this concept can be implemented using the subagent feature of Caude Code. 

the flow i've been thinking of:

1. claude code intance is invoked via sdk with a static workflow system prompt + a user prompt (the set up of the claude sdk needs to happen somehow programatically with python - look at @DOCS\claude_code_sdk.md)
2. the main agent has a subagent available to which is the meta agent.
3. the meta agent is responsible to do 2 things:
- generate a specialized subagent for the current task (dictated by the user prompt)
- generate set of tools (as mcp) for the specialized subagent to use
4. afetr completion, the main agent is responsible to call the specialized subagent to execute the task.
5. the main agent gets the response from the specialized subagent and returns it to the user.


what needs to happen (at least as draft):
- creating a workflow system prompt for the primary agent
- creating a workflow system prompt for the meta agent (that guide exaclty how to create another agent)
- creating a python script that will handle the main flow

please read .DOCS/* to get familar with the antropic eco system. 








